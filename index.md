# About
A seasoned Data/Software Engineer with nearly 8 years of experience in software development and over 6 years dedicated to data engineering and analysis.
I am skilled in building complex ETL pipelines, optimizing data processes, and leveraging cloud computing, with a strong focus on Microsoft Azure and Databricks.
As a passionate and continuous learner, I have developed robust analytical thinking and problem-solving skills, fostered by my university-level mathematical education encompassing a wide range of mathematical disciplines.
I am highly attentive to detail while always keeping the bigger picture in mind.
Motivated to solve complex and important problems, I am firmly committed to maintaining best practices.
I have worked with clients across various industries, including:
- oil and gas
- wholesale and distribution
- financial services
- telecommunications

**Top skills**
- Microsoft Azure
- Databricks
- Apache Spark
- SQL
- Python
- Scala
- Data Engineering
- Data Analysis
- Data Quality
- Data Modeling
- Problem-solving and analytical thinking skills in general

**Certifications**  
[Databricks Solutions Architect Champion](https://credentials.databricks.com/0d130a68-3355-4038-8b4d-ef9099daed66)  
[Databricks Certified Data Engineer Professional](https://credentials.databricks.com/4bf300c6-5027-4726-bf72-ca0fed8c8c69)  
[Databricks Certified Data Analyst Associate](https://credentials.databricks.com/44024e80-cce7-4a03-8b10-b0b0db5167ff)  
[Databricks Certified Data Engineer Associate](https://credentials.databricks.com/3b3d24e4-1ccc-45dc-b2e7-5eef5beca003)  
[Databricks Certified Associate Developer for Apache Spark 3.0](https://credentials.databricks.com/6804920f-5ef0-4c95-8687-23bbe897cbf7)  
Databricks Certified Associate Developer for Apache Spark 2.4 with Scala 2.11  
[Microsoft Certified: Azure Data Engineer Associate](https://www.credly.com/badges/26ed9e06-90a5-43b4-9995-e97839d53e4f)


[Databricks Partner Training - Data & AI Governance with Unity Catalog](https://credentials.databricks.com/c993173d-66be-41e1-aaab-2d07dbdb4d1f)  
[Databricks Solutions Architect Essentials](https://credentials.databricks.com/96e24028-b1a3-41aa-9db5-e489d333ab43)  
[Databricks Lakehouse Fundamentals](https://credentials.databricks.com/33230442-55be-4b4b-b03d-1647ecb8138a)  
[Microsoft Certified: Azure Data Fundamentals](https://www.credly.com/badges/70cb1019-0dfb-4aa6-834a-c4fc926dd675)  
[Microsoft Certified: Azure Fundamentals](https://www.credly.com/badges/4a39cb20-d86f-474f-b30c-b6164b9cc04e)


[Databricks Academy Accreditation - Azure Databricks Platform Architect](https://credentials.databricks.com/5c613351-fde9-4c8c-8bea-1b44afd8813d)  
[Databricks Academy Accreditation - Generative AI Fundamentals](https://credentials.databricks.com/a94f853e-f8a4-41cb-8124-8d9676234040)  
[Databricks Academy Accreditation - Platform Administrator](https://credentials.databricks.com/68d6de7b-1909-4854-9c0f-71bcbb665c08)

# Education
- **University**: Samara State Aerospace University  
  **Degree**: Master of Science - MS  
  **Field of study**: Mechanics and mathematical modeling  
  **Start date**: September 2015  
  **End date**: August 2017
- **University**: Samara State Aerospace University  
  **Degree**: Bachelor of Science - BS   
  **Field of study**: Applied Mathematics & Informatics    
  **Start date**: September 2011  
  **End date**: July 2015

# Experience and past projects
[//]: # (TODO: add Azure CLI, PowerShell maybe...)
[//]: # (TODO: add responsibilities)
[//]: # (TODO: add project descriptions)
[//]: # (TODO: add skills?)
[//]: # (TODO: use nested list for techtonoliges: Azure -> <particular azure service>)

## Lead Data Engineer

### Summary
**Company**: EPAM Systems  
**Company web-site**: [www.epam.com](https://www.epam.com)  
**Location**: Tbilisi, Georgia  
**Employment type**: Full-time  
**Start date**: May 2023    
**End date**: May 2024  
**Duration**: 1 year  
**Primary role**: Data Engineer  
**Secondary roles**: Data Analyst, Data Quality Engineer, Data Modeler  
**Title**: Lead Software Engineer  
**Domain**: wholesale, retail and distribution of electrical products, solutions, and related services

### Description
Data and analytics platform (Data Lake, Lakehouse).

### Responsibilities
- developing of new and support of existing complex ETL/ELT pipelines and PoCs applications
- extending and improvement of platform capabilities
- requirements gathering and clarification
- pipelines and data quality monitoring and automation
- performance optimization
- data modeling and architecture
- conducting code reviews and workshops on best practices
- tasks decomposition and delegation
- writing documentation
- mentoring of and leading other developers
- infrastructure management (occasionally)

### Participation highlights and achievements
- Implemented ETL pipelines with complex logic, formulas, and data structures.
- Automated data quality monitoring processes.
- Optimized the performance of ETL pipelines.
- Successfully refactored the project's codebase, achieving a substantial reduction in code volume (by a factor of 2 to 20), while enhancing maintainability and efficiency.
- Analyzed and discussed project requirements with stakeholders.
- Taught and enforced best practices in data engineering to the team.
- Mentored and consulted new team members on project specifics and technical challenges; conducted code reviews.
- Actively participated in meetings focused on architecture, data governance, and data modeling.
- Developed the "Product Alternatives" web application using the Flask, SQLAlchemy, and GraphQL frameworks (initially written with Streamlit + Pandas).
- Implemented two proof of concept (POC) projects involving Large Language Models (LLMs): a virtual analyst chatbot and feedback analysis pipeline.
- Made adjustments in CI/CD pipelines.

### Technical Details

#### Languages:
- Scala
- SQL
- Java
- Python
- JavaScript
- HTML
- CSS
- GraphQL
- YAML
- XML
- JSON

#### Development Tools:
- IntelliJ Idea
- GitHub Copilot
- SQL Server Management Studio
- Azure Storage Explorer
- Azure Data Studio
- Microsoft Excel

#### Tools/Technologies:
- Azure
  - Databricks
  - Data Factory
  - Functions
  - Data Lake Storage
  - SQL Database
  - Key Vault
  - App Service
  - Functions
  - Web Apps
  - Monitor
- Apache Spark
- Delta Lake
- Apache Hive
- OpenTelemetry

#### Libraries, frameworks:
- Flask
- SQLAlchemy
- Pandas

#### CI/CD, VCS, issue trackers:
- Azure DevOps
- Git
- Apache Maven
- Liquibase
- Atlassian Jira
- Atlassian Confluence

## Senior Data Engineer

### Summary
**Company**: Quantori  
**Company web-site**: [www.quantori.com](https://quantori.com/)  
**Location**: Tbilisi, Georgia  
**Employment type**: Full-time  
**Start date**: February 2023    
**End date**: April 2023  
**Duration**: 3 months   
**Primary role**: Data Engineer  
**Title**: Senior Software Engineer  
**Domain**: healthcare and life science

### Description
Automation of early diagnosis of cancer.
The budgets have not been approved and the project position has been cancelled.

### Participation highlights and achievements
- Self-development and education
  - certifications preparation
  - reading literature and documentation
  - competitive programming

## Data Engineer

### Summary
**Company**: DataArt  
**Company web-site**: [www.dataart.com](https://www.dataart.com/)  
**Location**: Tbilisi, Georgia  
**Employment type**: Full-time  
**Start date**: August 2022    
**End date**: January 2023  
**Duration**: 6 months  
**Primary role**: Data Engineer  
**Secondary roles**: Data Quality Engineer  
**Title**: Software Engineer  
**Domain**: financial services, insurance

### Description
Data and analytics platform (Data Lake, Lakehouse).

### Responsibilities
- developing of new and support of existing complex ETL/ELT pipelines
- requirements gathering and clarification
- conducting code reviews and workshops on best practices
- mentoring of and leading other developers

### Participation highlights and achievements
- Developed new data ingestion flows for various sources, including FTP, REST APIs, etc., using ELT/ETL processes, and implemented diverse data transformations.
- Performed automated data cleansing and implemented data quality checks.
- Enforced the usage of Apache NiFi best practices across the team, including naming conventions, dataflow structures, and the use of context parameters and variables.
- Documented AWA job deployment processes, identified problems and bottlenecks, and suggested improvements.
- Refactored, maintained, supported, deployed, and monitored existing AWA & Apache NiFi jobs.
- Initiated and participated in discussions regarding platform architecture and CI/CD processes, and advocated for the introduction of best practices (code reviews, version control, etc.).

### Technical Details

#### Languages:
- SQL
- Python
- Groovy

#### Development Tools:
- IntelliJ Idea

#### Tools/Technologies:
- Apache NiFi
- Automic Workload Automation (AWA)
- Bash
- Amazon S3

#### Frameworks, libraries
- JSON Jolt

#### CI/CD, VCS, issue trackers:
- Git
- NiFi Registry
- Automic Workload Automation (AWA) jobs
- Atlassian Jira

## Senior Data Engineer

### Summary
**Company**: EPAM Systems  
**Company web-site**: [www.epam.com](https://www.epam.com)  
**Location**: Minsk, Belarus  (Tbilisi, Georgia since March 2022)  
**Employment type**: Full-time  
**Start date**: October 2021    
**End date**: August 2022  
**Duration**: 11 months  
**Primary role**: Data Engineer  
**Secondary roles**: Data Analyst, Data Quality Engineer  
**Title**: Senior Software Engineer  
**Domain**: wholesale, retail and distribution of electrical products, solutions, and related services

### Description
Data and analytics platform (Data Lake, Lakehouse).

### Responsibilities
- developing of new and support of existing complex ETL/ELT pipelines and PoCs applications
- extending and improvement of platform capabilities
- requirements gathering and clarification
- pipelines and data quality monitoring and automation
- performance optimization
- data modeling and architecture
- conducting code reviews and workshops on best practices
- tasks decomposition and delegation
- writing documentation
- mentoring of and leading other developers
- infrastructure management (occasionally)

### Participation highlights and achievements
- Wrote, refactored, and migrated multiple ETL jobs and proof of concepts (POCs), including vendor profitability, and harmonization and unification of country-specific data.
- Reimplemented various standardized data lake layers (Landing, Raw, Refined, Prepared, Unified) and managed different load types (full, incremental, rolling period, SCD type 2).
- Optimized performance and reduced costs of ETL jobs and data lake storage structures.
- Extensively refactored code and resolved numerous bugs.
- Worked on data quality evaluation and validation.
- Implemented data flow monitoring.
- Conducted decomposition and delegation of large stories/tasks, and managed backlog population and refinement.
- Actively participated in discussions on platform design and evolution.
- Conducted a unified dimensional model (star schema) review, identified inconsistencies/discrepancies, and suggested improvements.
- Authored technical documentation and development guides.
- Mentored recently joined developers.
- Implemented automated testing for Spark ETL jobs (both end-to-end and unit testing).
- Implemented database schema evolution and versioning using Liquibase on Azure SQL Database and Azure Synapse Analytics.
- Participated in the development of CI/CD processes and pipelines for various environments.
- Led a team of three developers.

### Technical Details

#### Languages:
- Scala
- SQL
- Java
- Python
- YAML
- XML
- JSON

#### Development Tools:
- IntelliJ Idea
- SQL Server Management Studio
- Azure Storage Explorer
- Azure Data Studio
- Microsoft Excel

#### Tools/Technologies:
- Azure
  - Databricks
  - Synapse Analytics
  - Data Factory
  - Functions
  - Data Lake Storage
  - SQL Database
  - Key Vault
- Apache Spark
- Delta Lake
- Apache Hive

#### CI/CD, VCS, issue trackers:
- Azure DevOps
- Git
- Apache Maven
- Liquibase
- Atlassian Jira
- Atlassian Confluence

## Data Engineer

### Summary
**Company**: EPAM Systems  
**Company web-site**: [www.epam.com](https://www.epam.com)  
**Location**: Minsk, Belarus  
**Employment type**: Full-time  
**Start date**: May 2018    
**End date**: October 2021  
**Duration**: 3 years 6 months  
**Primary role**: Data Engineer  
**Secondary roles**: Data Analyst, Data Quality Engineer, Data Modeler  
**Title**: Software Engineer / Senior Software Engineer  
**Domain**: oil and gas industry

### Description
Data and analytics platform (Data Lake).

### Responsibilities
- developing of new and support of existing complex ETL/ELT pipelines and PoCs applications
- requirements gathering and clarification
- extending and improvement of platform capabilities
- pipelines and data quality monitoring and automation
- performance optimization
- data modeling
- conducting code reviews and workshops on best practices
- tasks decomposition and delegation
- infrastructure management (occasionally)

### Participation highlights and achievements
- Developed and generated numerous complex ETL jobs using Apache Velocity templates and a custom Maven plugin. This included ingestion of various data formats (Parquet, CSV, Avro, Excel spreadsheets using POI + Spark Datasource API, LAS text files, XML/WITSML files, HBase tables) and SFTP; SMB file downloading using Python.
- Extended and improved platform capabilities, including basic schema evolution and snapshot/delta merge.
- Implemented on-premise email notifications with attachments.
- Developed Proof of Concept (POC) dashboards using Excel and Power Pivot.
- Productionalized and enhanced a drilling performance optimization algorithm (transitioned from Python + Pandas to Scala + Spark).
- Collaborated with the customer to gather requirements, reverse-engineered complex financial calculations from Excel spreadsheets, and implemented them as automated ETL pipelines.
- Suggested, executed, and automated data quality estimations, metrics, and checks, utilizing Apache JMeter where applicable.
- Authored a test framework/library and automated tests for data visualization Spotfire dashboards in Python + Selenium.
- Migrated existing ETL jobs from SQL to Scala and assisted in the transition from on-premise Cloudera to Microsoft Azure Cloud.
- Optimized the performance of ETL jobs, including the development of a custom writer using a bulk SQL server driver and RDD (20x performance boost in comparison with JDBC).
- Developed Ansible playbooks for CI/CD pipelines and infrastructure deployment.

### Technical Details

#### Languages:
- Scala
- SQL
- Python
- Java
- Power Query/M Language (Microsoft Excel)
- DAX (Microsoft Excel)
- Visual Basic (Microsoft Excel)

#### Development Tools:
- IntelliJ Idea IDE
- Microsoft Excel

#### Tools/Technologies:
- Azure Databricks
- Azure Data Factory
- Azure Data Lake Storage
- Azure Synapse
- Azure SQL Database
- JMeter
- Selenium
- Apache Oozie
- Apache Sqoop
- Azure Logic App

#### Frameworks, libraries
- ScalaTest
- Apache POI

#### CI/CD, VCS, issue trackers:
- Azure DevOps
- Git
- Apache Maven
- Jenkins
- Nexus
- Artifactory (JFrog)
- Ansible
- Jira

## Data Engineer

### Summary

**Company**: EPAM Systems  
**Company web-site**: [www.epam.com](https://www.epam.com)  
**Location**: Minsk, Belarus  
**Employment type**: Full-time  
**Start date**: December 2017  
**End date**: May 2018  
**Duration**: 6 months  
**Primary Role**: Data Engineer  
**Title**: Software Engineer

### Description
Participated in intensive courses with a proctored assessment and implemented two practical capstone projects:
- 6-week mentored Elasticsearch & Apache Solr course
- 2-month mentored Big Data learning course

### Responsibilities
- gaining theoretical and practical experience in data engineering and analysis
- learning new skills, technologies, techniques, approaches, and best practices

### Participation highlights and achievements
- Set up locally running Elasticsearch and Solr clusters using Docker Swarm.
- Developed an email ingestion/archiving system on Spring Batch from SMTP to Elasticsearch and Solr, enabling full-text search and analytics on the contents of messages.

### Technical Details

#### Languages:
- Java
- Scala
- SQL

#### Development Tools:
- IntelliJ Idea IDE

#### Tools/Technologies:
- ElasticSearch
- Apache Solr
- Docker
- Apache Spark
- Apache Hadoop
- Apache Hive
- Apache Impala

#### Frameworks, libraries
- Spring Batch

#### CI/CD, VCS, issue trackers:
- Git
- Apache Maven
- Bash
- Jira

## Software Engineer

### Summary

**Company**: Netcracker Technology  
**Company web-site**: [www.netcracker.com](https://www.netcracker.com)  
**Location**: Samara, Russia  
**Employment type**: Full-time  
**Start date**: July 2016  
**End date**: December 2017  
**Duration**: 1 year 6 months  
**Primary Role**: Developer  
**Title**: Software Engineer  
**Domain**: Telecommunications, ISP/MSP

### Description
A BSS/OSS with numerous components and microservices, including:
- Historical data storage and archiving service (Oracle Database, WebLogic).
- Data transformation service (Spring, Elasticsearch, Logstash).
- System events dispatcher service (Spring, Apache Kafka).
- Reporting system (Pentaho BI, Docker, OpenShift).

### Responsibilities
- developing new and supporting existing features and products; bug fixing
- conducting root cause analysis; troubleshooting and debugging
- optimizing performance, including query and code profiling
- eliminating technical debt; code refactoring
- improving code coverage through tests
- configuring CI/CD processes and pipelines
- developing proofs of concept (PoCs)
- managing infrastructure on an occasional basis

### Participation highlights and achievements:
- Refactored the algorithm for merging sets of time intervals, reducing computational complexity to linear.
- Optimized the performance of SQL queries in Oracle Database 12c.
- Configured a custom Pentaho BI Docker image for deployment on OpenShift.
- Configured CI/CD pipelines using Jenkins, Maven, and Ant.
- Implemented new features as requested by end-users.
- Improved test coverage by developing multiple unit tests.
- Refactored legacy code to enhance cleanliness and maintainability.

### Technical details

#### Languages:
- Java
- Oracle PL/SQL
- Python (Jython)
- Groovy

#### Development tools:
- IntelliJ Idea IDE
- Oracle SQL Developer

#### CI/CD, VCS, issue trackers:
- Git
- Maven
- Jenkins
- Bash
- Ant
- SVN

#### Frameworks, libraries:
- Spring
- Google Guice
- Log4j
- EasyMock
- Mockito
- JMockit
- PowerMock
- JUnit
- Jython

[//]: # (# Personal projects and pet projects)
[//]: # (## spark-commons)
[//]: # (## Real estate data scraping and analysis)
[//]: # (Add  others)